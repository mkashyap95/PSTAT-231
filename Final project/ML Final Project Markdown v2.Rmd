---
title: "Madhu Final Project"
output: html_document
date: "2022-12-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(tidyselect)
library(corrplot)
library(ggplot2)
library(GGally)
library(reticulate)
library(tidymodels)
library(tidyverse)
library(janitor)
library(corrplot)
library(rpart.plot)
library(randomForest)
library(vip)
library(caret)
library(broom)
library(boot)
library(data.table)
library(mlr)
library(ClusterR)
library(cluster)
library(factoextra)


setwd("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\processed")
df_comp <- read.csv("cleaned.csv")
# 
# # r bin the ratings into 5 buckets
# df_comp$Stars = cut(df_comp$rating_average, breaks = seq(0,10,2), labels = c(1,2,3,4,5))
#write.csv(df_comp, "C:\\Users\\bleec\\OneDrive\\Documents\\Personal\\Madhu\\ML Class Fall 2022\\cleaned_v2.csv")
#reticulate::repl_python()
```

```{r}
# one hot encode the domains and mechanics columns
# Step 1: Convert strings to lists
convert_str_to_list <- function(x){
  c(strsplit(x, ", "))
}
df_comp$mechanics <- lapply(df_comp$mechanics, convert_str_to_list)
df_comp$domains <- lapply(df_comp$domains, convert_str_to_list)

# Step 2: Get all unique values
unique_mechanics = c()
for (row in 1:nrow(df_comp)) {
   unique_mechanics = union(unique_mechanics,unlist(df_comp[row, "mechanics"]))
}

unique_domains = c()
for (row in 1:nrow(df_comp)) {
   unique_domains = union(unique_domains,unlist(df_comp[row, "domains"]))
}

# Step 3: Create dummy columns
prep_column_name <- function(x){
  paste0("Domain ", x)
}
unique_domain_cols <- lapply(unique_domains, prep_column_name)
prep_column_name <- function(x){
  paste0("Mechanic ", x)
}
unique_mechanic_cols <- lapply(unique_mechanics, prep_column_name)

df_comp[unlist(unique_domain_cols)] <- 0
df_comp[unlist(unique_mechanic_cols)] <- 0

# Step 4: Populate dummy columns
for (row in 1:nrow(df_comp)) {
  dom_vec = unlist(df_comp[row, "domains"])
  for (dom_idx in 1:length(dom_vec)){
    dom_col = paste0("Domain ", dom_vec[dom_idx])
    df_comp[row, dom_col] = 1
  }
  
  mech_vec = unlist(df_comp[row, "mechanics"])
  for (mech_idx in 1:length(mech_vec)){
    mech_col = paste0("Mechanic ", mech_vec[mech_idx])
    df_comp[row, mech_col] = 1
  }
}

# Step 5: Drop one column to avoid dummy variable trap
# Drop the least populated column in each
min_sum_mech = nrow(df_comp)
min_idx_mech = 0
for (col_idx in 1:length(unique_mechanic_cols)){
  sum_ = sum(df_comp[, unique_mechanic_cols[col_idx][[1]]])
  if (sum_<min_sum_mech){
    min_idx_mech = col_idx
    min_sum_mech = sum_
  }
}

min_sum_dom = nrow(df_comp)
min_idx_dom = 0
for (col_idx in 1:length(unique_domain_cols)){
  sum_ = sum(df_comp[, unique_domain_cols[col_idx][[1]]])
  if (sum_<min_sum_dom){
    min_idx_dom = col_idx
    min_sum_dom = sum_
  }
}

drop <- c(unique_domain_cols[min_idx_dom][[1]],unique_mechanic_cols[min_idx_mech][[1]])
df_final = df_comp[,!(names(df_comp) %in% drop)]

df_final <- apply(df_final,2,as.character)
#write.csv(df_final, "C:\\Users\\bleec\\OneDrive\\Documents\\Personal\\Madhu\\ML Class Fall 2022\\cleaned_final.csv", , row.names = FALSE)

```

```{r}
df_final <- read.csv("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\processed\\cleaned_final.csv")

drop <- c("year_published", "id", "name", "Stars", "owned_users", "mechanics", "domains","bgg_rank")
df = df[,!(names(df) %in% drop)]

#splitting data
set.seed(1212)
df_split <- initial_split(df, strata = "rating_average")

df_train <- training(df_split)
df_test <- testing(df_split)

# https://juliasilge.com/blog/xgboost-tune-volleyball/
#parameter grid
form = as.formula(paste("rating_average", "~", paste(colnames(df), collapse='+')))

df_fold <- vfold_cv(df_train, v=5, strata = "rating_average")
```

```{r}
# training XGB boost for regression
#model
xgb_spec <- boost_tree(
  trees = tune(), 
  tree_depth = tune(), 
  min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune(),                         ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  trees(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train),
  learn_rate(),
  size = 20
)

#workflow
boost_wf <- workflow() %>%
  add_formula(form) %>%
  add_model(xgb_spec)


xgb_res <- tune_grid(
  boost_wf,
  resamples = df_fold,
  grid = xgb_grid,
  control = control_grid(verbose=T)
)
```



```{r}
# training Random forest for predicting rating_average
#model
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

p_grid <- grid_regular(mtry(range = c(1,8)),trees(range = c(100,500)), 
                       min_n(range = c(1,10)), levels = 6)

#workflow
rf_tree_wf <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_formula(form)

tune_param <- tune_grid(
  rf_tree_wf, 
  resamples = df_fold, 
  grid = p_grid, 
  control = control_grid(verbose = T)
)

autoplot(tune_param)
```



```{r}
#model
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>% 
  set_mode("regression")

#workflow
tree_wf <- workflow() %>% 
  add_model(tree_spec %>% set_args(cost_complexity = tune())) %>% 
  add_formula(form)

#tuning cost_complexity
set.seed(1212)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  tree_wf, 
  resamples = df_fold, 
  grid = param_grid,
  control = control_grid(verbose = T)
)

autoplot(tune_res)
```

```{r}
# Train linear regression model to predict rating_average
simple_recipe <- recipe(form, data = df_train)

#dummy coding categorical predictors
final_recipe <- simple_recipe %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors())

lm_model <- linear_reg() %>% 
  set_engine("lm")

lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(final_recipe)

summary(lm_wflow)

#fitting to model and recipe
lm_fit <- fit(lm_wflow, df_train)
lm_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

#training RMSE
lm_train_res <- predict(lm_fit, new_data = df_train %>% select(-rating_average))
lm_train_res %>% 
  head()

#reattaching column of actual observed age
lm_train_res <- bind_cols(lm_train_res, df_train %>% select(rating_average))
lm_train_res %>% 
  head()

#metrics
lm_metrics <- metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae)
lm_metrics(lm_train_res, truth = rating_average, estimate = .pred)

#plotting predicted vs. observed
lm_train_res %>% 
  ggplot(aes(x=.pred, y=jitter(rating_average))) +
  geom_point(alpha = 0.2) +
  geom_abline(lty=2) +
  theme_classic() +
  coord_obs_pred()

```


```{r}
# training k-means clustering for recommendation engine (unsupervised learning)
# first, we are going to apply PCA

df <- read.csv("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\processed\\cleaned_final.csv")

df <- df[,c("year_published", "min_players", "max_players", "play_time", "min_age", "users_rated", "rating_average", "bgg_rank", "complexity_average", "game_age")]

res.pca <- prcomp(df, scale = TRUE)
fviz_eig(res.pca)

df_transform = as.data.frame(-res.pca$x[,1:2])

# https://www.geeksforgeeks.org/k-means-clustering-in-r-programming/
# https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/#computing-k-means-clustering-in-r
# https://medium.com/@zullinira23/implementation-of-principal-component-analysis-pca-on-k-means-clustering-in-r-794f03ec15f

#fviz_nbclust(df_transform, kmeans, method = 'wss')
# alternative methods
fviz_nbclust(df_transform, kmeans, method = "silhouette")

# set k based on the above determination of optimal clusters
k = 2
kmeans_post_pca = kmeans(df_transform, centers = 2, nstart = 50)
fviz_cluster(kmeans_post_pca, data = df_transform)

distance_mat <- dist(df, method = 'euclidean')
#distance_mat

# try hierchial clustering for comparison
Hierar_cl <- hclust(distance_mat, method = "average")
#Hierar_cl

plot(Hierar_cl)

abline(h = 110, col = "green")
 
# Cutting tree by no. of clusters
fit <- cutree(Hierar_cl, k = 3 )
#fit
 
table(fit)
rect.hclust(Hierar_cl, k = 3, border = "green")
```

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv("C:\\Users\\bleec\\OneDrive\\Documents\\Personal\\Madhu\\ML Class Fall 2022\\cleaned_v2.csv")

sns.set(rc={"axes.facecolor":"#FCE4DE","figure.facecolor":"#CABFC1"})
hue_C = ["#615154", "#F7B4A7", "#94DDDE", "#F0ABC1", "#B46B82"]
sns.pairplot(data,hue= "Stars", palette= hue_C)
plt.show()
```

