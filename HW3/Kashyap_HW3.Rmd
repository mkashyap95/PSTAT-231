---
title: "Kashyap_HW3"
author: "Madhuri"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_floating: true
---

## Homework 3

```{r setup, include=FALSE}
#library(usethis)
#gitcreds::gitcreds_set()
knitr::opts_chunk$set(echo = TRUE)

library(tidymodels)
library(tidyverse)
library(discrim) # for discriminant analysis models
library(poissonreg) # for poisson regression
library(corrr) # for visualizing correlation matrices
library(klaR) # for naive bayes
library(ggplot2) # for plots
library(ggthemes)
library(yardstick)
tidymodels_prefer()

#data
df <- read.csv("train.csv")

#converting to fcators
df$Survived <- as.factor(df$Survived)
df$Pclass <- as.factor(df$Pclass)
df$Sex <- as.factor(df$Sex)

#changing reference group
new_df <- within(df, Survived <- relevel(Survived, ref = 1))

#checking frequencies
table(df$Survived)
table(df$Pclass)
table(df$Sex)
```
### Question 1

```{r}
set.seed(1104)
#splitting data
df_split <- initial_split(df, prop = 0.7, strata = Survived)
df_train <- training(df_split)
df_test <- testing(df_split)
```

The training dataset has 623 observations (70% of 891 is 623), and the test dataset has
268 observations (the remaining 30%). So, the two datasets have the appropriate number
of observations.

There are missing data in the training dataset, especially for the age variable.

It is a good idea to use stratified sampling for this data because stratified sampling makes it so that each group is accurately represented in the data. Also, stratified sampling on the outcome allows for the outcome to be distributed the same in both the training and testing datasets. That is, the proportions for survived and not survived will be the same in both datasets.


### Question 2

```{r}
# check frequency in training set
table(df_train$Survived)
##
## Yes No
## 239 384
# make a bar plot
df_train %>%
  ggplot(aes(x = Survived)) +
  geom_bar() +
  scale_x_discrete(labels = c("No","Yes"))
```

In the training set, 239 people survived and 384 people did not survive.

### Question 3

```{r}
# visualize correlation matrix
cor_train <- df_train %>%
select(Age, SibSp, Parch, Fare) %>%
correlate()
## Correlation computed with
## • Method: 'pearson'
## • Missing treated using: 'pairwise.complete.obs'
rplot(cor_train)

# create heatmap style correlation plot
cor_train %>%
  stretch() %>%
  ggplot(aes(x, y, fill = r)) +
  geom_tile() +
  geom_text(aes(label = as.character(fashion(r))))
```


From both visualizations, we see that some variables are slightly correlated with one
others (correlations ranged from -.31 to .37). The largest correlation is between parch
and sib_sp (r = .37) - the more parents and children you had on board the Titanic, the more
siblings and spouses you had on board with you as well. Age was negatively correlated with
both parch (r = -.15) and sib_sp (r = -.31), suggesting that the older people were, the less parents, children, siblings, and spouses they had on board. Fare was positively correlated with parch (r = .24) and sib_sp (r = .18), which means that people with more family members on board paid higher fares. Lastly, age was positively correlated with fare - the older people were, the more they paid. It is important to note that these visualizations don’t tell us if these correlations are significant.


### Question 4

```{r}
# start the recipe
titanic_recipe <- recipe(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare,
                         data = df_train)
# get the information of the recipe
titanic_rec_info <- summary(titanic_recipe)
titanic_rec_info

# add steps
titanic_rec <- titanic_recipe %>%
  step_impute_linear(Age) %>% # deal with missing values from age
  step_dummy(Pclass) %>% # dummy code categorical predictors
  step_dummy(Sex) %>%
  step_interact(terms = ~ starts_with("Sex"):Fare) %>% # create interaction terms
  step_interact(terms = ~ Age:Fare)
# provide the summary of the recipe
titanic_rec
```


### Question 5

```{r}
# specify logistic regression
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
# create workflow
log_flow <- workflow() %>%
  add_model(log_reg) %>%
  add_recipe(titanic_rec)

# apply workflow to training data
log_fit <- fit(log_flow, df_train)
# view the results
log_fit %>% tidy()
```


### Question 6

```{r}
# specify LDA
lda_mod <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")
# create workflow
lda_flow <- workflow() %>%
  add_model(lda_mod) %>%
  add_recipe(titanic_rec)
# apply workflow to training data
lda_fit <- fit(lda_flow, df_train)
```


### Question 7 

```{r}
# specify QDA
qda_mod <- discrim_quad() %>%
  set_mode("classification") %>%
  set_engine("MASS")

# create workflow
qda_flow <- workflow() %>%
  add_model(qda_mod) %>%
  add_recipe(titanic_rec)

# apply workflow to training data
qda_fit <- fit(qda_flow, df_train)
```


### Question 8

```{r}
# specify naive bayes
nb_mod <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("klaR") %>%
  set_args(usekernel = FALSE) # assume predictors are drawn from Gaussian distribution

# create workflow
nb_flow <- workflow() %>%
  add_model(nb_mod) %>%
  add_recipe(titanic_rec)

# apply workflow to training data
nb_fit <- fit(nb_flow, df_train)
```


### Question 9

#### Logistic Regression

```{r}
# use the model to predict probabilities for the training data
log_reg_pred <- predict(log_fit, new_data = df_train, type = "prob")
log_reg_pred


# generate the confusion matrix
augment(log_fit, new_data = df_train) %>%
  conf_mat(truth = Survived, estimate = .pred_class)

# create a visual representation of confusion matrix
augment(log_fit, new_data = df_train) %>%
  conf_mat(truth = Survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

# calculate the accuracy
log_reg_acc <- augment(log_fit, new_data = df_train) %>%
  accuracy(truth = Survived, estimate = .pred_class)
log_reg_acc
```


#### LDA

```{r}
# use the model to predict probabilities for the training data
lda_pred <- predict(lda_fit, new_data = df_train, type = "prob")
lda_pred

# generate the confusion matrix
augment(lda_fit, new_data = df_train) %>%
  conf_mat(truth = Survived, estimate = .pred_class)

# create a visual representation of confusion matrix
augment(lda_fit, new_data = df_train) %>%
  conf_mat(truth = Survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

# calculate the accuracy
lda_acc <- augment(lda_fit, new_data = df_train) %>%
  accuracy(truth = Survived, estimate = .pred_class)
lda_acc
```


#### QDA

```{r}
# use the model to predict probabilities for the training data
qda_pred <- predict(qda_fit, new_data = df_train, type = "prob")
qda_pred

# generate the confusion matrix
augment(qda_fit, new_data = df_train) %>%
  conf_mat(truth = Survived, estimate = .pred_class)

# create a visual representation of confusion matrix
augment(qda_fit, new_data = df_train) %>%
  conf_mat(truth = Survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

# calculate the accuracy
qda_acc <- augment(qda_fit, new_data = df_train) %>%
  accuracy(truth = Survived, estimate = .pred_class)
qda_acc
```


#### Naive Bayes

```{r message=FALSE, warning=FALSE}
# use the model to predict probabilities for the training data
nb_pred <- predict(nb_fit, new_data = df_train, type = "prob")
nb_pred

# generate the confusion matrix
augment(nb_fit, new_data = df_train) %>%
  conf_mat(truth = Survived, estimate = .pred_class)

# create a visual representation of confusion matrix
augment(nb_fit, new_data = df_train) %>%
  conf_mat(truth = Survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

# calculate the accuracy
nb_acc <- augment(nb_fit, new_data = df_train) %>%
  accuracy(truth = Survived, estimate = .pred_class)
nb_acc
```


```{r}
# provide a table of the predicted results for each of the four models
pred_table <- bind_cols(log_reg_pred, lda_pred, qda_pred, nb_pred)
pred_table

# compare model performance
accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, qda_acc$.estimate, nb_acc$.estimate)
models <- c("Logistic Regression", "LDA", "QDA", "Naive Bayes")
results <- tibble(accuracies = accuracies, models = models)
results %>% arrange(-accuracies)
```


Two models achieved the highest accuracy - 0.8186 & 0.8089 - logistic regression and linear
discriminant analysis.

I will fit the testing data with the logistic regression model since it is simpler.


### Question 10

```{r}
# use the model to predict probabilities for the testing data
predict(log_fit, new_data = df_test, type = "prob")

# generate the confusion matrix
augment(log_fit, new_data = df_test) %>%
  conf_mat(truth = Survived, estimate = .pred_class)

# create a visual representation of confusion matrix
augment(log_fit, new_data = df_test) %>%
  conf_mat(truth = Survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

# assess accuracy, sensitivity, and specificity
multi_metric <- metric_set(accuracy, sensitivity, specificity)

augment(log_fit, new_data = df_test) %>%
  multi_metric(truth = Survived, estimate = .pred_class)

# create a ROC curve - Yes
augment(log_fit, new_data = df_test) %>%
  roc_curve(Survived, .pred_1) %>%
  autoplot()

# create a ROC curve - No
augment(log_fit, new_data = df_test) %>%
  roc_curve(Survived, .pred_0) %>%
  autoplot()
```


The logistic regression model predicted the testing data with 82.46% accuracy.

The logistic regression model performed well on the testing data. The training and testing
accuracies differed - the training accuracy is higher than the testing one. This might be the case if the model does not capture the patterns that are in the population data and is overfitting to the trainign data, but I don’t think the accuracies are too different.


