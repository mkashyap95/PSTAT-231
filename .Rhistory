tidy()
#creating recipe using training data
simple_abalone_recipe <- recipe(age ~ sex + length + diameter + ht + whole_wt +
shucked_wt + viscera_wt + shell_wt,
data = abalone_train)
simple_abalone_recipe
#dummy coding categorical predictors
abalone_recipe <- simple_abalone_recipe %>%
step_dummy(all_nominal_predictors()) %>%
step_interact(terms = ~ sex_M:shucked_wt) %>%
step_interact(terms = ~ length:diameter) %>%
step_interact(terms = ~ shucked_wt:shell_wt) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors())
abalone_recipe
#creating and storing linear regression object
lm_model <- linear_reg() %>%
set_engine("lm")
#empty workflow
lm_wflow <- workflow() %>%
add_model(lm_model) %>%
add_recipe(abalone_recipe)
summary(lm_wflow)
#fitting to model and recipe
lm_fit <- fit(lm_wflow, abalone_train)
lm_fit %>%
extract_fit_parsnip() %>%
tidy()
df <- read.csv("Pokemon.csv")
View(df)
install.packages("janitor")
library(janitor)
View(df)
?clean_names
clean_names(df)
df_new <- clean_names(df)
View(df_new)
df_new <- as_tibble(df_new) %>%
clean_names()
library(tidymodels)
library(tidyverse)
df_new <- as_tibble(df_new) %>%
clean_names()
View(df_new)
df_clean <- as_tibble(df) %>%
clean_names()
library(janitor)
library(tidymodels)
library(tidyverse)
df <- read.csv("Pokemon.csv")
df_clean <- as_tibble(df) %>%
clean_names()
View(df_clean)
library(ggplot2)
#barplot
ggplot(df_clean, aes(x = factor(type_1))) +
geom_bar(stat = "bin", fill = "steelblue") +
theme_classic()
#barplot
ggplot(df_clean, aes(x = factor(type_1))) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic()
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic()
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic() +
coord_flip()
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic() +
coord_flip() +
xlab("Primary Type")
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic() +
coord_flip() +
xlab("Primary Type") +
ylab("Count")
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic() +
coord_flip() +
xlab("Primary Type") +
ylab("Count") +
title(main = "Frequency of primary types of pokemon")
?title
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic() +
coord_flip() +
xlab("Primary Type") +
ylab("Count")
summary(df_clean$type_1)
summary(factor(df_clean$type_1))
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic() +
coord_flip() +
xlab("Primary Type") +
ylab("Count")
#number of classes =18
summary(factor(df_clean$type_1))
?filter
#filtering data
df_filtered <- filter(df_clean, type_1 == "Bug" | type_1 == "Fire" | type_1 == "Grass" |
type_1 == "Normal" | type_1 == "Water" | type_1 == "Psychic")
View(df_filtered)
#checking new dataframe
summary(df_filtered$type_1)
#checking new dataframe
summary(factor(df_filtered$type_1))
#converting to factors
df_filtered %>%
as.factor(type_1) %>%
as.factor(legendary)
#converting to factors
df_filtered %>%
as.factor(type_1) +
as.factor(legendary)
#converting to factors
df_filtered %>%
as.factor(type_1)
#converting to factors
df_filtered$type_1 %>%
as.factor()
#converting to factors
df_filtered$type_1 <- as.factor(df_filtered$type_1)
df_filtered$legendary <- as.factor(df_filtered$legendary)
set.seed(1212)
set.seed(1212)
df_split <- initial_split(df_filtered, strata = "type_1")
df_train <- training(df_split)
df_test <- testing(df_split)
View(df_train)
View(df_test)
341/458
set.seed(1212)
df_split <- initial_split(df_filtered, strata = "type_1")
df_train <- training(df_split)
df_test <- testing(df_split)
df_fold <- vfold_cv(df_train, v = 5, strata = "type_1")
View(df_fold)
View(df_fold[[1]][[1]])
df_fold
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic() +
coord_flip() +
xlab("Primary Type") +
ylab("Count")
#number of classes =18
summary(factor(df_clean$type_1))
#filtering data
df_filtered <- filter(df_clean, type_1 == "Bug" | type_1 == "Fire" | type_1 == "Grass" |
type_1 == "Normal" | type_1 == "Water" | type_1 == "Psychic")
#checking new dataframe
summary(factor(df_filtered$type_1))
#converting to factors
df_filtered$type_1 <- as.factor(df_filtered$type_1)
df_filtered$legendary <- as.factor(df_filtered$legendary)
library(caret)
install.packages("caret")
library(caret)
library(broom)
?dplyr
x <- c(rep(1, 337), rep(0, 421))
resamples <- lapply(1:1000, function(i) sample(x, replace = T))
boot.mean <- sapply(resamples, mean)
boot.mean
hist(boot.mean)
abline(v=mean(quantile(boot.mean, probs = 0.005),col="red",lwd=12))
abline(v=mean(quantile(boot.mean, probs = 0.995),col="red",lwd=12))
# 99% Confidence Interval:
c(quantile(boot.mean, probs = 0.005), quantile(boot.mean, probs = 0.995))
### Another way to get the 99% confidence interval using boot package ###
boot_mean_func <- function(orig_vector, resample_vector) {
mean(orig_vector[resample_vector])
}
# R is number of replications
mean_results <- boot(x, boot_mean_func, R = 1000)
library(boot)
x <- c(rep(1, 337), rep(0, 421))
resamples <- lapply(1:1000, function(i) sample(x, replace = T))
boot.mean <- sapply(resamples, mean)
boot.mean
hist(boot.mean)
abline(v=mean(quantile(boot.mean, probs = 0.005),col="red",lwd=12))
abline(v=mean(quantile(boot.mean, probs = 0.995),col="red",lwd=12))
# 99% Confidence Interval:
c(quantile(boot.mean, probs = 0.005), quantile(boot.mean, probs = 0.995))
### Another way to get the 99% confidence interval using boot package ###
boot_mean_func <- function(orig_vector, resample_vector) {
mean(orig_vector[resample_vector])
}
# R is number of replications
mean_results <- boot(x, boot_mean_func, R = 1000)
tidy(mean_results)
boot.ci(mean_results, conf=0.99)
#recipe
simple_rec1 <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense +
hp + sp_def, data = df_train)
?step_dummy
pokemon_recipe <- simple_rec1 %>%
step_dummy(legendary) %>%
step_dummy(generation) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors())
pokemon_recipe
?multinom_reg
reg1 <- multinom_reg(0 < mixture < 1, penalty = 0) %>%
reg1 <- multinom_reg(mixture = 0.5, penalty = 0) %>%
set_engine("glmnet")
reg1 <- multinom_reg(mixture = 0.5, penalty = tune()) %>%
set_engine("glmnet")
#workflow
elastic_workflow <- workflow() %>%
add_recipe(pokemon_recipe) %>%
add_model(reg1)
#tuning penalty
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)
penalty_grid
#tuning penalty
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 10)
penalty_grid
?grid_regular
mixture_grid <- grid_random(mixture(range = c(0,1)), levels = 10)
mixture_grid <- grid_random(mixture(range = c(0,1)), size = 10)
mixture_grid
#model
reg1 <- multinom_reg(mixture = tune(), penalty = tune()) %>%
set_engine("glmnet")
#workflow
elastic_workflow <- workflow() %>%
add_recipe(pokemon_recipe) %>%
add_model(reg1)
#tuning penalty
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 10)
penalty_grid
mixture_grid <- grid_random(mixture(range = c(0,1)), size = 10)
mixture_grid
?grid
?tune_grid
#combining grids to one dataframe
grid_df <- penalty_grid + mixture_grid
View(grid_df)
?as.data.frame
#combining grids to one dataframe
grid_df <- c(penalty_grid,mixture_grid)
#combining grids to one dataframe
grid_df <- as.data.fram(c(penalty_grid,mixture_grid))
#combining grids to one dataframe
grid_df <- as.data.frame(c(penalty_grid,mixture_grid))
#fitting model
tuned_mod <- tune_grid(elastic_workflow,
resamples = df_fold,
grid = grid_df)
install.packages("glmnet")
install.packages("glmnet")
install.packages("glmnet")
#fitting model
tuned_mod <- tune_grid(elastic_workflow,
resamples = df_fold,
grid = grid_df)
#recipe
simple_rec1 <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense +
hp + sp_def, data = df_fold)
#recipe
simple_rec1 <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense +
hp + sp_def, data = df_train)
#barplot
ggplot(df_clean, aes(x = type_1)) +
geom_bar(stat = "count", fill = "steelblue") +
theme_classic() +
coord_flip() +
xlab("Primary Type") +
ylab("Count")
#number of classes =18
summary(factor(df_clean$type_1))
#filtering data
df_filtered <- filter(df_clean, type_1 == "Bug" | type_1 == "Fire" | type_1 == "Grass" |
type_1 == "Normal" | type_1 == "Water" | type_1 == "Psychic")
#checking new dataframe
summary(factor(df_filtered$type_1))
#converting to factors
df_filtered$type_1 <- as.factor(df_filtered$type_1)
df_filtered$legendary <- as.factor(df_filtered$legendary)
set.seed(1212)
df_split <- initial_split(df_filtered, strata = "type_1")
df_train <- training(df_split)
df_test <- testing(df_split)
df_fold <- vfold_cv(df_train, v = 5, strata = "type_1")
View(df_train)
View(df_fold)
df_fold
set.seed(1212)
df_split <- initial_split(df_filtered, strata = "type_1")
df_train <- training(df_split)
df_test <- testing(df_split)
df_fold <- vfold_cv(df_train, v = 5, strata = "type_1")
pokemon_recipe
#model
reg1 <- multinom_reg(mixture = tune(), penalty = tune()) %>%
set_engine("glmnet")
#workflow
elastic_workflow <- workflow() %>%
add_recipe(pokemon_recipe) %>%
add_model(reg1)
#tuning penalty
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 10)
penalty_grid
mixture_grid <- grid_random(mixture(range = c(0,1)), size = 10)
mixture_grid
#combining grids to one dataframe
grid_df <- as.data.frame(c(penalty_grid,mixture_grid))
#fitting model
tuned_mod <- tune_grid(elastic_workflow,
resamples = df_fold,
grid = grid_df)
tuned_mod
autoplot(tuned_mod)
show_notes(.Last.tune.result)
View(df_train)
df_filtered$generation <- as.factor(df_filtered$generation)
set.seed(1212)
df_split <- initial_split(df_filtered, strata = "type_1")
df_train <- training(df_split)
df_test <- testing(df_split)
df_fold <- vfold_cv(df_train, v = 5, strata = "type_1")
#recipe
simple_rec1 <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense +
hp + sp_def, data = df_train)
pokemon_recipe <- simple_rec1 %>%
step_dummy(legendary) %>%
step_dummy(generation) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors())
pokemon_recipe
#model
reg1 <- multinom_reg(mixture = tune(), penalty = tune()) %>%
set_engine("glmnet")
#workflow
elastic_workflow <- workflow() %>%
add_recipe(pokemon_recipe) %>%
add_model(reg1)
#tuning penalty
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 10)
penalty_grid
mixture_grid <- grid_random(mixture(range = c(0,1)), size = 10)
mixture_grid
#combining grids to one dataframe
grid_df <- as.data.frame(c(penalty_grid,mixture_grid))
#fitting model
tuned_mod <- tune_grid(elastic_workflow,
resamples = df_fold,
grid = grid_df)
tuned_mod
autoplot(tuned_mod)
?autoplot
collect_metrics(tuned_mod)
#best model
best_fit <- select_best(tuned_mod, metric = "rsq")
?select_best
#best model
best_fit <- select_best(tuned_mod)
autoplot(tuned_mod)
collect_metrics(tuned_mod)
?autoplot
collect_metrics(tuned_mod, metric = "accuracy")
collect_metrics(tuned_mod)
#best model
best_fit <- select_best(tuned_mod, metric = "accuracy")
best_fit
#best model
best_fit <- select_best(tuned_mod, metric = "roc_a")
#best model
best_fit <- select_best(tuned_mod, metric = "roc_auc")
best_fit
#finalizing workflow
final_mod <- finalize_workflow(elastic_workflow, best_fit)
#final fit
final_fit <- fit(final_mod, data = df_train)
#applying to testing data
augment(final_fit, new_data = df_test)
#applying to testing data
augment(final_fit, new_data = df_test) %>%
rsq(truth = Salary, estimate = .pred)
#applying to testing data
augment(final_fit, new_data = df_test) %>%
rsq(truth = type_1, estimate = .pred)
#applying to testing data
augment(final_fit, new_data = df_test)
#applying to testing data
augment(final_fit, new_data = df_test) %>%
rsq(truth = type_1, estimate = .pred_class)
?rsq
#applying to testing data
test_mod <- augment(final_fit, new_data = df_test)
View(test_mod)
#applying to testing data
augment(final_fit, new_data = df_test) %>%
rsq(truth = type_1, estimate = .pred_class)
?augment
#applying to testing data
augment(final_fit, new_data = df_test) %>%
multi_metric(truth = type_1, estimate = .pred_class) ##google this with wifi
?multi_metric
??multi_metric
#applying to testing data
augment(final_fit, new_data = df_test) %>%
conf_mat(truth = type_1, estimate = .pred_class) ##google this with wifi
#applying to testing data
augment(final_fit, new_data = df_test) %>%
roc_auc(truth = type_1, estimate = .pred_class) ##google this with wifi
#applying to testing data
augment(final_fit, new_data = df_test) %>%
accuracy(truth = type_1, estimate = .pred_class) ##google this with wifi
#applying to testing data
test_mod <- augment(final_fit, new_data = df_test) %>%
accuracy(truth = type_1, estimate = .pred_class) ##google this with wifi
View(test_mod)
#applying to testing data
test_mod <- augment(final_fit, new_data = df_test)
accuracy(test_mod,truth = type_1, estimate = .pred_class) ##google this with wifi
conf_mat(test_mod, truth = type_1, estimate = .pred_class)
View(df_test)
#roc curve for training data
roc_curve(final_fit, type_1, .pred_class)
#roc curve for training data
augment(final_fit, new_data = df_train) %>%
roc_curve(final_fit, type_1, .pred_class)
#roc curve for training data
augment(final_fit, new_data = df_train) %>%
roc_curve(type_1, .pred_class)
#roc curve for training data
augment(final_fit, new_data = df_train) %>%
roc_auc(type_1, .pred_class)
#roc curve for training data
augment(final_fit, new_data = df_train) %>%
roc_auc(type_1, .pred_Bug:.pred_Water)
#roc curve for training data
augment(final_fit, new_data = df_train) %>%
roc_auc(type_1, .pred_Bug:.pred_Water)
#roc for testing data
roc_auc(test_mod, type_1, .pred_Bug:.pred_Water)
#roc for testing data
roc_curve(test_mod, type_1, .pred_Bug:.pred_Water)
#roc curve for training data
augment(final_fit, new_data = df_train) %>%
roc_curve(type_1, .pred_Bug:.pred_Water)
#roc curves for each type
roc_curve(test_mod, type_1, .pred_Bug:.pred_Water) %>%
autoplot()
#roc for testing data
roc_curve(test_mod, type_1, .pred_Bug:.pred_Water) %>%
autoplot()
#roc for testing data
roc_curve(test_mod, type_1, .pred_Bug:.pred_Water)
#roc curve for training data
augment(final_fit, new_data = df_train) %>%
roc_auc(type_1, .pred_Bug:.pred_Water)
#roc curves for each type
roc_curve(test_mod, type_1, .pred_Bug:.pred_Water) %>%
autoplot()
conf_mat(truth = type_1, estimate = .pred_class) %>%
autoplot(type = "heatmap")
#heatmap
test_mod %>%
conf_mat(truth = type_1, estimate = .pred_class) %>%
autoplot(type = "heatmap")
#roc curve for training data
augment(final_fit, new_data = df_train) %>%
roc_auc(type_1, .pred_Bug:.pred_Water)
#roc for testing data
roc_auc(test_mod, type_1, .pred_Bug:.pred_Water)
#roc curves for each type
roc_curve(test_mod, type_1, .pred_Bug:.pred_Water) %>%
autoplot()
#heatmap
test_mod %>%
conf_mat(truth = type_1, estimate = .pred_class) %>%
autoplot(type = "heatmap")
roc_auc(test_mod,truth = type_1, estimate = .pred_class)
roc_auc(test_mod,truth = type_1, estimate = .pred_Bug:.pred_Water)
accuracy(test_mod, truth = type_1, estimate = .pred_Bug:.pred_Water)
accuracy(test_mod, truth = type_1, estimate = .pred_class)
#heatmap
test_mod %>%
conf_mat(truth = type_1, estimate = .pred_class) %>%
autoplot(type = "heatmap")
?autoplot
autoplot(tuned_mod)
autoplot(tuned_mod, plotTable = TRUE)
autoplot(tuned_mod, ... = "Proportion of Lasso Penalty")
autoplot(tuned_mod, type = "marginals")
autoplot(tuned_mod, type = "performance")
autoplot(tuned_mod, type = "parameters")
autoplot(tuned_mod, type = "marginals")
autoplot(tuned_mod, ... = format())
autoplot(tuned_mod, format())
autoplot(tuned_mod, format(penalty))
autoplot(tuned_mod, format("penalty"))
autoplot(tuned_mod, format(penalty_grid))
