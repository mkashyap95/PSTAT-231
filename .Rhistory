tree_depth(),
min_n(),
trees(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), df_train),
learn_rate(),
size = 1
)
#workflow
boost_wf <- workflow() %>%
add_formula(form) %>%
add_model(xgb_spec)
xgb_res <- tune_grid(
boost_wf,
resamples = df_fold,
grid = xgb_grid,
metrics = metric_set(yardstick::rmse),
control = control_grid(verbose=T)
)
simple_recipe <- recipe(form, data = df_train)
#dummy coding categorical predictors
final_recipe <- simple_recipe %>%
step_dummy(all_nominal_predictors()) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors())
lm_model <- linear_reg() %>%
set_engine("lm")
lm_wflow <- workflow() %>%
add_model(lm_model) %>%
add_recipe(final_recipe)
summary(lm_wflow)
#fitting to model and recipe
lm_fit <- fit(lm_wflow, df_train)
#model
tree_spec <- decision_tree() %>%
set_engine("rpart") %>%
set_mode("regression")
#workflow
tree_wf <- workflow() %>%
add_model(tree_spec %>% set_args(cost_complexity = tune())) %>%
add_formula(form)
#tuning cost_complexity
set.seed(1212)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
tune_res <- tune_grid(
tree_wf,
resamples = df_fold,
grid = param_grid,
metrics = metric_set(yardstick::rmse),
control = control_grid(verbose = T)
)
#reading in final data
df_final <- read.csv("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\processed\\cleaned_final.csv")
drop <- c("year_published", "id", "name", "Stars", "owned_users", "mechanics", "domains","bgg_rank")
df = df[,!(names(df) %in% drop)]
#splitting data
set.seed(1212)
df_split <- initial_split(df_final, strata = "rating_average")
df_train <- training(df_split)
df_test <- testing(df_split)
#parameter grid
form = as.formula(paste("rating_average", "~", paste(colnames(df_final), collapse='+')))
df_fold <- vfold_cv(df_train, v=5, strata = "rating_average")
simple_recipe <- recipe(form, data = df_train)
#dummy coding categorical predictors
final_recipe <- simple_recipe %>%
step_dummy(all_nominal_predictors()) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors())
lm_model <- linear_reg() %>%
set_engine("lm")
lm_wflow <- workflow() %>%
add_model(lm_model) %>%
add_recipe(final_recipe)
summary(lm_wflow)
#fitting to model and recipe
lm_fit <- fit(lm_wflow, df_train)
#reading in final data
df_final <- read.csv("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\processed\\cleaned_final.csv")
drop <- c("year_published", "id", "name", "Stars", "owned_users", "mechanics", "domains","bgg_rank")
df = df[,!(names(df) %in% drop)]
#splitting data
set.seed(1212)
df_split <- initial_split(df_final, strata = "rating_average")
df_train <- training(df_split)
df_test <- testing(df_split)
#parameter grid
form = as.formula(paste("rating_average", "~", paste(colnames(df_final), collapse='+')))
df_fold <- vfold_cv(df_train, v=5, strata = "rating_average")
#model
tree_spec <- decision_tree() %>%
set_engine("rpart") %>%
set_mode("regression")
#workflow
tree_wf <- workflow() %>%
add_model(tree_spec %>% set_args(cost_complexity = tune())) %>%
add_formula(form)
#tuning cost_complexity
set.seed(1212)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
tune_res <- tune_grid(
tree_wf,
resamples = df_fold,
grid = param_grid,
metrics = metric_set(yardstick::rmse),
control = control_grid(verbose = T)
)
#reading in final data
df_final <- read.csv("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\processed\\cleaned_final.csv")
drop <- c("year_published", "id", "name", "Stars", "owned_users", "mechanics", "domains","bgg_rank")
df_final = df_final[,!(names(df) %in% drop)]
#splitting data
set.seed(1212)
df_split <- initial_split(df_final, strata = "rating_average")
df_train <- training(df_split)
df_test <- testing(df_split)
#parameter grid
form = as.formula(paste("rating_average", "~", paste(colnames(df_final), collapse='+')))
df_fold <- vfold_cv(df_train, v=5, strata = "rating_average")
#reading in final data
df_final <- read.csv("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\processed\\cleaned_final.csv")
drop <- c("year_published", "id", "name", "Stars", "owned_users", "mechanics", "domains","bgg_rank")
df_final = df_final[,!(names(df_final) %in% drop)]
#splitting data
set.seed(1212)
df_split <- initial_split(df_final, strata = "rating_average")
df_train <- training(df_split)
df_test <- testing(df_split)
#parameter grid
form = as.formula(paste("rating_average", "~", paste(colnames(df_final), collapse='+')))
df_fold <- vfold_cv(df_train, v=5, strata = "rating_average")
simple_recipe <- recipe(form, data = df_train)
#dummy coding categorical predictors
final_recipe <- simple_recipe %>%
step_dummy(all_nominal_predictors()) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors())
lm_model <- linear_reg() %>%
set_engine("lm")
lm_wflow <- workflow() %>%
add_model(lm_model) %>%
add_recipe(final_recipe)
summary(lm_wflow)
#fitting to model and recipe
lm_fit <- fit(lm_wflow, df_train)
lm_fit %>%
extract_fit_parsnip() %>%
tidy()
#training RMSE
lm_train_res <- predict(lm_fit, new_data = df_train %>% select(-rating_average))
lm_train_res %>%
head()
#reattaching column of actual observed age
lm_train_res <- bind_cols(lm_train_res, df_train %>% select(rating_average))
lm_train_res %>%
head()
#metrics
lm_metrics <- metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae)
lm_metrics(lm_train_res, truth = rating_average, estimate = .pred)
#plotting predicted vs. observed
lm_train_res %>%
ggplot(aes(x=.pred, y=jitter(rating_average))) +
geom_point(alpha = 0.2) +
geom_abline(lty=2) +
theme_classic() +
coord_obs_pred()
#model
tree_spec <- decision_tree() %>%
set_engine("rpart") %>%
set_mode("regression")
#workflow
tree_wf <- workflow() %>%
add_model(tree_spec %>% set_args(cost_complexity = tune())) %>%
add_formula(form)
#tuning cost_complexity
set.seed(1212)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
tune_res <- tune_grid(
tree_wf,
resamples = df_fold,
grid = param_grid,
metrics = metric_set(yardstick::rmse),
control = control_grid(verbose = T)
)
autoplot(tune_res)
# training XGB boost for regression
#model
xgb_spec <- boost_tree(
trees = tune(),
tree_depth = tune(),
min_n = tune(),
loss_reduction = tune(),                     ## first three: model complexity
sample_size = tune(),
mtry = tune(),         ## randomness
learn_rate = tune(),                         ## step size
) %>%
set_engine("xgboost") %>%
set_mode("regression")
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
trees(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), df_train),
learn_rate(),
size = 1
)
#workflow
boost_wf <- workflow() %>%
add_formula(form) %>%
add_model(xgb_spec)
xgb_res <- tune_grid(
boost_wf,
resamples = df_fold,
grid = xgb_grid,
metrics = metric_set(yardstick::rmse),
control = control_grid(verbose=T)
)
View(tune_res)
tune_res %>%
collect_metrics() %>%
arrange(mean)
best_penalty <- select_best(tune_res, metric = "rmse")
best_penalty
1+!
tune_res %>%
collect_metrics() %>%
arrange(mean)
tune_res %>%
collect_metrics() %>%
arrange(mean)
best_penalty <- select_best(tune_res, metric = "rmse")
best_penalty
tree_final <- finalize_workflow(tree_wf, best_penalty)
tree_final_fit <- fit(tree_final, data = df_train)
tree_final_fit
augment(tree_final_fit, new_data = df_train) %>%
rmse(truth = rating_average, estimate=.pred)
collect_metrics(tune_res) %>% arrange(mean)
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("regression")
p_grid <- grid_regular(mtry(range = c(1,8)),trees(range = c(100,500)),
min_n(range = c(1,10)), levels = 2)
#workflow
rf_tree_wf <- workflow() %>%
add_model(rf_spec) %>%
add_formula(form)
tune_param <- tune_grid(
rf_tree_wf,
resamples = df_fold,
grid = p_grid,
metrics = metric_set(yardstick::rmse),
control = control_grid(verbose = T)
)
autoplot(tune_param)
tune_param
autoplot(tune_param)
tune_param %>%
collect_metrics() %>%
arrange(mean)
best_rf <- select_best(tune_param, metric = "rmse")
best_rf
rf_final <- finalize_workflow(rf_tree_wf, best_rf)
rf_final_fit <- fit(rf_final, data = df_train)
rf_final_fit
augment(rf_final_fit, new_data = df_train) %>%
rmse(truth = rating_average, estimate=.pred)
collect_metrics(tune_param) %>% arrange(mean)
# training XGB boost for regression
#model
xgb_spec <- boost_tree(
trees = tune(),
tree_depth = tune(),
min_n = tune(),
loss_reduction = tune(),                     ## first three: model complexity
sample_size = tune(),
mtry = tune(),         ## randomness
learn_rate = tune(),                         ## step size
) %>%
set_engine("xgboost") %>%
set_mode("regression")
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
trees(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), df_train),
learn_rate(),
size = 1
)
#workflow
boost_wf <- workflow() %>%
add_formula(form) %>%
add_model(xgb_spec)
xgb_res <- tune_grid(
boost_wf,
resamples = df_fold,
grid = xgb_grid,
metrics = metric_set(yardstick::rmse),
control = control_grid(verbose=T)
)
xgb_res
autoplot(xgb_res)
xgb_res
#autoplot(xgb_res)
xgb_res %>%
collect_metrics() %>%
arrange(mean)
best_boost <- select_best(xgb_res, metric = "rmse")
best_boost
boost_final <- finalize_workflow(boost_wf, best_boost)
boost_final_fit <- fit(boost_final, data = df_train)
boost_final_fit
augment(boost_final_fit, new_data = df_train) %>%
rmse(truth = rating_average, estimate=.pred)
collect_metrics(xgb_res) %>% arrange(mean)
model_types = c("pruned tree","random forest","xgboost","linear regression")
tune_results = lst(tune_res, tune_param, xgb_res, lm_train_res)
best_model = lst(best_penalty, best_rf, best_boost, lm_train_res)
final_models = lst(class_tree_final_fit, rf_tree_final_fit, boost_final_fit, lm_fit)
model_types = c("pruned tree","random forest","xgboost","linear regression")
tune_results = lst(tune_res, tune_param, xgb_res, lm_train_res)
best_model = lst(best_penalty, best_rf, best_boost, lm_train_res)
final_models = lst(tree_final_fit, rf_final_fit, boost_final_fit, lm_fit)
rmses = c()
for (idx in 1:length(tune_results)){
performance_tbl <- collect_metrics(tune_results[[idx]]) %>% arrange(mean)
best_rmse <- (filter(performance_tbl, .config == best_model[[idx]]['1', '.config'][[1]]))$mean
rmses <- c(rmses,best_rmse)
}
model_types = c("pruned tree","random forest","xgboost","linear regression")
final_models = lst(tree_final_fit, rf_final_fit, boost_final_fit, lm_fit)
rmses = c(0.09, 0.36, 0.02, 0.56)
# put the model types and their ROC AUC's into one table
combined <- as_tibble(cbind(model_types,rmses))
combined
# using the table of rocaucs, find the model type with the best roc auc,
# and set "best_model" equal to that model's final workflow
best_model <- final_models[which.min(combined$rmses)]
best_model
#re-fit on test data
test_mod <- augment(best_model[[1]], new_data = df_test)
test_mod
test_mod %>%
rmse(truth = rating_average, estimate=.pred)
#heatmap
test_mod %>%
conf_mat(truth = rating_average, estimate = .pred) %>%
autoplot(type = "heatmap")
test_mod %>%
conf_mat(truth = rating_average, estimate = .pred) %>%
autoplot(type = "heatmap")
augment(boost_final_fit, new_data = df_train) %>%
ggplot(aes(ratings_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
augment(boost_final_fit, new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
augment(test_mod, new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
#heatmap
augment(boost_final_fit, new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
#heatmap
augment(best_model, new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
augment(best_model[[1]], new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
augment(test_mod, new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
augment(best_mod[[1]], new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
#heatmap
augment(best_model[[1]], new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
vip(boost_final_fit)
model_final <- finalize_workflow(boost_tree_wf, best_boost)
model_final <- finalize_workflow(boost_wf, best_boost)
model_final_fit <- fit(model_final, data = df_train)
model_final_fit
augment(model_final_fit, new_data = df_train) %>%
rmse(truth = rating_average, estimate = .pred)
#rsq for training
augment(model_final_fit, new_data = df_train) %>%
rsq(truth = rating_average, estimate = .pred)
#rmse for testing data
augment(model_final_fit, new_data = df_test) %>%
rmse(truth = rating_average, estimate = .pred)
#rsq for testing
augment(model_final_fit, new_data = df_test) %>%
rsq(truth = rating_average, estimate = .pred)
augment(best_model[[1]], new_data = df_test) %>%
ggplot(aes(rating_average, .pred)) +
geom_abline() +
geom_point(alpha = 0.5)
#rmse for training data
augment(model_final_fit, new_data = df_train) %>%
rmse(truth = rating_average, estimate = .pred)
#rmse for testing data
augment(model_final_fit, new_data = df_test) %>%
rmse(truth = rating_average, estimate = .pred)
# training k-means clustering for recommendation engine (unsupervised learning)
# first, we are going to apply PCA
df <- read.csv("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\processed\\cleaned_final.csv")
df <- df[,c("year_published", "min_players", "max_players", "play_time", "min_age", "users_rated", "rating_average", "bgg_rank", "complexity_average", "game_age")]
# First, perform PCA to reduce dimensionality and expedite k-means clustering & plotting
res.pca <- prcomp(df, scale = TRUE)
# plot the pca results
fviz_eig(res.pca)
df_transform = as.data.frame(-res.pca$x[,1:2])
fviz_nbclust(df_transform, kmeans, method = "silhouette")
# set k based on the above determination of optimal clusters
k = 2
kmeans_post_pca = kmeans(df_transform, centers = 2, nstart = 50)
# it appears there are two relatively distinct clusters
fviz_cluster(kmeans_post_pca, data = df_transform)
# Calculate a distance matrix to be used to hierachial clustering
distance_mat <- dist(df, method = 'euclidean')
# try hierchial clustering for comparison
Hierar_cl <- hclust(distance_mat, method = "average")
plot(Hierar_cl)
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(Hierar_cl, k = 3 )
table(fit)
rect.hclust(Hierar_cl, k = 3, border = "green")
# it appears that the hierachial clustering is not a good fit for our dataset
library(janitor)
library(tidyverse)
library(tidyselect)
library(dplyr)
library(corrplot)
library(ggplot2)
library(GGally)
library(reticulate)
library(dplyr)
library(tidyselect)
library(corrplot)
library(ggplot2)
library(GGally)
library(reticulate)
library(tidymodels)
library(tidyverse)
library(janitor)
library(corrplot)
library(rpart.plot)
library(randomForest)
library(vip)
library(caret)
library(broom)
library(boot)
library(data.table)
library(mlr)
library(ClusterR)
library(cluster)
library(factoextra)
setwd("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\unprocessed")
df <- read.csv("bgg_dataset.csv", sep = ";")
# alternative hierchial clustering
# Calculate a distance matrix to be used to hierachial clustering
distance_mat <- dist(df_transform, method = 'euclidean')
# try hierchial clustering for comparison
Hierar_cl <- hclust(distance_mat, method = "average")
plot(Hierar_cl)
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(Hierar_cl, k = 3 )
table(fit)
rect.hclust(Hierar_cl, k = 3, border = "green")
# it appears that the hierachial clustering is not a good fit for our dataset
library(janitor)
library(tidyverse)
library(tidyselect)
library(dplyr)
library(corrplot)
library(ggplot2)
library(GGally)
library(reticulate)
library(tidymodels)
library(rpart.plot)
library(randomForest)
library(vip)
library(caret)
library(broom)
library(boot)
library(data.table)
library(mlr)
library(ClusterR)
library(cluster)
library(factoextra)
setwd("C:\\Users\\kashyap\\Desktop\\PSTAT-231\\Final project\\data\\unprocessed")
df <- read.csv("bgg_dataset.csv", sep = ";")
