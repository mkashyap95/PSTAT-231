---
title: "Kashyap_HW6"
author: "Madhuri"
date: "`r Sys.Date()`"
output: 
  html_document:
  toc: true
  toc_float: true
  code_float: true
---


## Homework 6

```{r setup, message=FALSE}
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(janitor)
library(corrplot)
library(rpart.plot)
library(randomForest)
library(vip)
library(caret)
library(broom)
library(boot)

pokemon <- read.csv("Pokemon.csv")
```


### Exercise 1

```{r}
pokemon_clean <- as_tibble(pokemon) %>% 
  clean_names()

#visualizing types
ggplot(pokemon_clean, aes(x = type_1)) +
  geom_bar(stat = "count", fill = "steelblue") +
  theme_classic() +
  coord_flip() +
  xlab("Primary Type") +
  ylab("Count") 

#filtering data
df_filtered <- filter(pokemon_clean, type_1 == "Bug" | type_1 == "Fire" | type_1 == "Grass" 
                      | type_1 == "Normal" | type_1 == "Water" | type_1 == "Psychic")

#checking new dataframe
summary(factor(df_filtered$type_1))

#converting to factors
df_filtered$type_1 <- as.factor(df_filtered$type_1)
df_filtered$legendary <- as.factor(df_filtered$legendary)
df_filtered$generation <- as.factor(df_filtered$generation)

#splitting data
set.seed(1212)
df_split <- initial_split(df_filtered, strata = "type_1")

df_train <- training(df_split)
df_test <- testing(df_split)

#v-fold
df_fold <- vfold_cv(df_train, v = 5, strata = "type_1")

#recipe
simple_rec1 <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense +
                        hp + sp_def, data = df_train) 

pokemon_recipe <- simple_rec1 %>% 
  step_dummy(legendary) %>% 
  step_dummy(generation) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors())
pokemon_recipe
```


### Exercise 2

```{r}
summary(df_train)

mat <- select(df_train,total:speed)
mat %>% 
  cor() %>% 
  corrplot(type = "lower", diag = F, method = 'color')
```


We see that there are no negative correlations which fits with intuition. We also see that the total score is strongly correlated with special attack, and defense, and attack which we would expect as these scores contribute directly to the total score.


### Exercise 3

```{r, message=FALSE}
#model
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>% 
  set_mode("classification")

#workflow
tree_wf <- workflow() %>% 
  add_model(tree_spec %>% set_args(cost_complexity = tune())) %>% 
  add_formula(type_1 ~ legendary + generation + sp_atk + attack + speed + 
                defense + hp + sp_def)

#tuning cost_complexity
set.seed(1212)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  tree_wf, 
  resamples = df_fold, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
tune_res

#visualizing
autoplot(tune_res)
```

We see that smaller complexity penalties yield higher model ROC AUC values, and these taper off severely once penalty values are greater than 0.05. Single decision trees therefore perform better with smaller complexity penalty values. 


### Exercise 4

```{r}
tune_res %>% 
  collect_metrics() %>% 
  arrange(mean)

best_penalty <- select_best(tune_res, metric = "roc_auc")
best_penalty
```

The best-performing model has an ROC AUC value of 0.64.


### Exercise 5

```{r}
#finalizing workflow and fit
class_tree_final <- finalize_workflow(tree_wf, best_penalty)
class_tree_final_fit <- fit(class_tree_final, data = df_train)

augment(class_tree_final_fit, new_data = df_train) %>%
  roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)

#visualizing the tree
class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```


### Exercise 5

```{r, message=FALSE}
#model
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

p_grid <- grid_regular(mtry(range = c(1,8)),trees(range = c(100,500)), 
                       min_n(range = c(1,10)), levels = 8)

#workflow
rf_tree_wf <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_formula(type_1 ~ legendary + generation + sp_atk + attack + speed + 
                defense + hp + sp_def)
```


The mtry parameter allows for a random selection of predictor variables (and is therefore dependent on the number of columns), trees specifies the number of decision trees to be built, and min_m defines the minimum number of data points on node or decision point, that will be required to split the node further. 
The mtry parameter can't be less than 1 since that's the minimum number of predictors needed for the model to run, and not more than 8 as our recipe defines a maximum of 8 predictor variables. Setting mtry to 8 would test a model similar to the one we've tested above with all of the predictor variables being used in the tree-based model (which may yield redundancies).


### Exercise 6

```{r, message=FALSE}
tune_param <- tune_grid(
  rf_tree_wf, 
  resamples = df_fold, 
  grid = p_grid, 
  metrics = metric_set(roc_auc),
  control = control_grid(verbose = T)
)

autoplot(tune_param)
```


the results show that generally, a lower number of predictors yield better performing models. However, the number of trees and node size seem to have rather variable effects, although it does seem that larger node sizes are performing slightly better. It's important to note here that these models don't vary tremendously across each of the parameters based on the ROC AUC values observed (ranges 0.65-0.73).


### Exercise 7

```{r}
tune_param %>% 
  collect_metrics() %>% 
  arrange(mean)

best_params <- select_best(tune_param, metric = "roc_auc")
best_params
```


The ROC AUC of the best model is 0.73.


### Exercise 8

```{r}
#finalizing workflow and fit
rf_tree_final <- finalize_workflow(rf_tree_wf, best_params)
rf_tree_final_fit <- fit(rf_tree_final, data = df_train)
rf_tree_final_fit

augment(rf_tree_final_fit, new_data = df_train) %>%
  roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)

rf_tree_final_fit %>% 
  extract_fit_parsnip() %>% 
  vip()

df_train %>% 
  group_by(type_1) %>% 
  summarise(mean = mean(sp_atk), n =n())
```

These results indicate that the special attack value is the most useful while legendary status is the least useful in predicting type of Pokemon. This is not entirely what would be expected since a pokemon's legendary status would significantly help in streamlining the type of pokemon bu only if they are indeed legendary. The importance of the special attack makes intuitive sense since looking at these values by type, there does look to be some significant differences.


### Exercise 9

```{r, message=FALSE}
#model
boost_spec <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

#workflow
boost_wf <- workflow() %>% 
  add_model(boost_spec) %>% 
  add_formula(type_1 ~ legendary + generation + sp_atk + attack + speed + 
                defense + hp + sp_def)

#parameter grid
set.seed(1212)
boost_grid <- grid_regular(trees(range = c(10,2000)), levels = 10)

#tuning
tune_boost <- tune_grid(
  boost_wf, 
  resamples = df_fold, 
  grid = boost_grid, 
  metrics = metric_set(roc_auc),
  control = control_grid(verbose = T)
)

#visual
autoplot(tune_boost)

#best model
best_boost <- select_best(tune_boost, metric = "roc_auc")
best_boost

boost_final <- finalize_workflow(boost_wf, best_boost)
boost_final_fit <- fit(boost_final, data = df_train)
boost_final_fit

augment(boost_final_fit, new_data = df_train) %>%
  roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)

collect_metrics(tune_boost) %>% arrange(mean)
```


We see here that the ROC AUC value increases as the number of trees increases. However, setting trees too high can lead to over fitting issues.
The ROC AUC value of the best performing model is 0.71.


### Exercise 10

```{r}
model_types = c("pruned tree","random forest","boosted tree")
tune_results = lst(tune_res, tune_param, tune_boost)
best_model = lst(best_penalty, best_params, best_boost)
final_models = lst(class_tree_final_fit, rf_tree_final_fit, boost_final_fit)

rocauc = c()

for (idx in 1:length(tune_results)){
  performance_tbl <- collect_metrics(tune_results[[idx]]) %>% arrange(mean)
  best_rocauc <- (filter(performance_tbl, .config == best_model[[idx]]['1', '.config'][[1]]))$mean
  rocauc <- c(rocauc,best_rocauc)
}

# put the model types and their ROC AUC's into one table
combined <- as_tibble(cbind(model_types,rocauc))
combined

# using the table of rocaucs, find the model type with the best roc auc,
# and set "best_model" equal to that model's final workflow
best_model <- final_models[which.max(combined$rocauc)]
best_model

#re-fit on test data
test_mod <- augment(best_model[[1]], new_data = df_test)
test_mod
  
#ROC_AUC
test_mod %>% 
  roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)

#ROC curves
test_mod %>% 
  roc_curve(truth = type_1, estimate = .pred_Bug:.pred_Water) %>% 
  autoplot()

#heatmap
test_mod %>% 
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```


The random forest model performed best on the folds.
The model looks to be best at predicting the Normal & Psychic types, while performing pretty poorly on classifying most other classes, with the worst one being Water types. This is likely to the disproportionate number of Water type Pokemon present in the data set.


### Exercise 11

```{r, message=FALSE}
#setup
data_input <- read.csv("abalone.csv")
data <- as_tibble(data_input) %>%
  mutate(age = rings+1.5)  %>%
  mutate(type = factor(type))  %>%
  select(-rings)

#splitting data
set.seed(3435)
data_split <- initial_split(data, strata = "age")

data_train <- training(data_split)
data_test <- testing(data_split)

#model
abalone_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

#tuning grid
ab_grid <- grid_regular(mtry(range = c(1,8)),trees(range = c(100,500)), 
                       min_n(range = c(1,10)), levels = 8)

#workflow
abalone_wf <- workflow() %>% 
  add_model(abalone_spec) %>% 
  add_formula(age ~ longest_shell + diameter + height + whole_weight + shucked_weight + 
                viscera_weight + shell_weight + type)

#v-fold for cross-alidation
data_fold <- vfold_cv(data_train, v=5, strata = "age")

#tuning
tune_abalone <- tune_grid(
  abalone_wf, 
  resamples = data_fold, 
  grid = ab_grid, 
  metrics = metric_set(rsq),
  control = control_grid(verbose = T)
)

#visualizing
autoplot(tune_abalone)

#best model
ab_best <- select_best(tune_abalone, metric = "rsq")

#finalizing workflow 
ab_final <- finalize_workflow(abalone_wf, ab_best)
ab_final_fit <- fit(ab_final, data = data_train)
ab_final_fit

#rmse for training data
augment(ab_final_fit, new_data = data_train) %>%  
  rmse(truth = age, estimate = .pred) 
#rsq for training
augment(ab_final_fit, new_data = data_train) %>% 
  rsq(truth = age, estimate = .pred)

#rmse for testing data
augment(ab_final_fit, new_data = data_test) %>%
  rmse(truth = age, estimate = .pred)
#rsq for testing
augment(ab_final_fit, new_data = data_test) %>%
  rsq(truth = age, estimate = .pred)

collect_metrics(tune_abalone) %>% arrange(mean)
```


The best model to maximize RMSE has 2 predictors, 271 tress, and a minimum node size of 10 which yields a mean of RMSE = 0.56
Fitting this model on the training data, we see the RMSE = 1.25 & R^2 = 0.87
Refitting this model on testing data, RMSE = 2.21 & R^2 = 0.55 suggesting that we may be encountering some severe overfitting issues.
